<html>
<head>
<title>Tian Theunissen</title>
<meta property="og:image" content="profile_fig.png" />
<link rel="stylesheet" href="style.css">
<style>

.papers li {
  margin: 10px 0;
}

.hl {
    background-color: rgb(255, 255, 208);
    /* list-style-position: inside; */
    padding: 5px;
}

p.quote {
    padding: 10px;
}

.container {
    display: flex;
    justify-content: center;
}
.main {
    /* border: 1px solid black; */
    max-width: 900px;
}

.columns {
    display: flex;
    /* padding: 30px; */
}

.column{
    /* margin-left: 30px; */
    margin-right: 30px;
}
.columnb {
    flex : 1;
    margin-right: 0px;
}
.columnc {
    flex : 1;
    margin-left: 20px;
}

header {
    /* border: 1px solid gray; */
    display: flex;
}
header .desc{
    margin-right: 30px;
    max-width: 400px;
}

.headshot {
    width: 256px;
    height: 256px;
    display:block;
    background: url("./me_grey.jpg") no-repeat;
    background-size: 256px;
}
.headshot:hover {
    background: url("./imgs/mri_slice0.png") no-repeat;
    background-size: 256px;
}
</style>

</head>
<body>


<div class="container">
<div class="main">

<header>
    <div class="column">
    <img src="./profile_fig.png" width="384px">
    </div>
    <div class="desc">
        <h1>Tian Theunissen</h1>
        I'm a PhD student studying generalization in deep learning.
		I was educated as a computer and electronic engineer and naturally drifted towards machine learning.

<p>
Some <b>simple questions</b> I ask myself: 
	Why do big neural networks generalize? 
	Does the bias-variance tradeoff need to be modified or replaced? 
	Do bigger neural networks actually perform better under early-stopping conditions? 
	Are neural networks just ensembles of cleaverly fitted subpredictors?
	Why is there so little general theory on ensembling?
        <p>
        <a href="https://scholar.google.com/citations?user=p3bOWQEAAAAJ&hl=en">
            <b>[publications]</b></a>
            &emsp; &emsp;
        <a href="http://engineering.nwu.ac.za/must">
            <b>[group]</b></a>
            &emsp; &emsp;
        <code>tiantheunissen@gmail.com</code><br>
    </div>
</header>

<div class="row">
<hr>
<h2>
    My current goings-on:
</h2>
<ul>
<li>
	Late in 2020 I am writing up my thesis.
</li>
<li>
    Early in 2020 I went to AAAI in New York to present a paper we wrote in our research group.
</li>
<li>
	Late in 2019 I presented a paper at the first SACAIR conference in Cape Town, South Africa.
</li>
<li>
	Late in 2018 I presented some of my work at a workshop in Hermanus, South Africa.
</li>
</ul>
<hr>
</div>



<div class="columns">
<div class = "columnb">
<h2>Research</h2>
Without proven fundamental principles in machine learning we are forced to resort to heuristics and scientific rigor.
As a result I do a lot of experimenting on toy problems while making sure my assumptions are reasonable.
<p>
<!-- <h3>Noise and generalization</h3> -->
<ul class="papers">
	
	<li class="hl">
		<a href="https://arxiv.org/pdf/2001.06178.pdf">
			<b>Benign interpolation of noise in deep learning</b></a><br>
		Marthinus W. Theunissen, Marelie H.Davel, Etienne Barnard
		<br>
		<i>To be published. Dec 2020.</i>
	</li>
		
	<li>
		<a href="http://www.cair.org.za/sites/default/files/2020-01/theunissen-2019-insights-regarding-overfitting.pdf">
			<b>Insights regarding overfitting on noise in deeplearning</b></a><br>
		Marthinus W. Theunissen, Marelie H.Davel, Etienne Barnard
		<br>
		<i>Published 2019.</i>
	</li>
		
	<li class="hl">
		<a href="https://arxiv.org/pdf/2001.06178.pdf">
			<b>DNNs as layers of cooperating classifiers</b></a><br>
		Marelie H.Davel, Marthinus W. Theunissen, Arnold M. Pretorius, Etienne Barnard
		<br>
		<i>Published. 2020.</i>
	</li>
	
</ul>
</div>

<div class = "columnc">
<figure>
<a href="https://mltheory.org/deep.pdf">
    <img src="imgs/deep_img.png", width=100%>
    <figcaption>Deep Double Descent</figcaption>
</a>
</figure>

<figure>
<a href="https://arxiv.org/abs/1905.11604">
    <img src="imgs/sgd1.png", width=100%>
    <figcaption>Dynamics of SGD</figcaption>
</a>
</figure>

<figure>
<a href="misc/gauss">
    <img src="misc/gauss/display_image.png", width=100%>
    <figcaption>Gauss's Principle of Least Action</figcaption>
</a>
</figure>

<!-- <figure>
<a href="misc/gauss">
    <img src="imgs/manifold_img.svg", width=100%>
    <figcaption>Gauss's Principle of Least Action</figcaption>
</a>
</figure> -->

<h2>About Me</h2>
I did my undergrad in EECS at UC Berkeley.
I'm broadly interested in theory and science.

<p>
In the past, I have interned at OpenAI (with Ilya Sutskever)
and Google Research (with Raziel Alvarez),
and have also done research in error-correcting codes, distributed storage,
and cryptography. I am partially supported by a Google PhD Fellowship,
and I am grateful for past support from NSF GRFP.

<p>
See also my <a href="./index_old.html">old website</a> for more.
This version borrowed in part from <a href="https://lucatrevisan.github.io/">Luca
    Trevisan</a> and <a href="https://jonbarron.info/">Jon Barron</a>.

<h2 style="margin-top:30pt">What People are Saying</h2>

<p class="quote" style="text-align: right;">
<i>a "high-level" scientist</i> &nbsp; &mdash;colleague (ML)
</p>

<p class="quote">
<i>makes plots and draws lines through them</i>
<br>
&nbsp; &nbsp;
&nbsp; &nbsp;
&nbsp; &nbsp;
&mdash;colleague (TCS)
</p>

<p class="quote" style="text-align: right;">
<i>has merits that outweigh flaws</i> &nbsp; &mdash;reviewer 2
</p>

</div>
</div>

</div>
</div>




</div>

</body>

</html>
